Machine Learning Force Field
========================================

Machine Learning Force Field (MLFF) platform aims at generating force fields with accuracy comparable to Ab Initio DFT molecular dynamics. It provides 8 types of features with translation, rotation, and permutation invariance. It also supports 4 engines for training and prediction, which are: 

        1. Linear Model

        2. Nonlinear Model

        3. Kalman Filter-based Neural Netowrk (KFNN)

        4. Kalman Filter-based DeepMD (KF-DeepMD)

Theoretical Backgrounds
-----------------------
| Phys. Rev. Lett. 98, 146401
| Phys. Rev. B 99, 064103  
| ...

Environment Setup 
-----------------

We recommmend using anaconda for package installing and management. You should first install Anaconda following the steps shown on its official website. Please don't attempt to use miniconda instead of Anaconda, since it might incur various dependecy and path errors that are hard to debug. 

To install conda environment, use the following command. You may choose a new version of Anaconda. 

::

    wget https://repo.anaconda.com/archive/Anaconda3-2020.07-Linux-x86_64.sh

Then, create a new environment for this module. 

::
    
    conda create -n mlff python=3.8

After mlff has been created, re-enter the current environment. 
    
:: 
    
    conda deactivate
    conda activate mlff

You should identify the architecture of your Nvidia GPU and install a compatible pytorch version accordingly. We use RTX 3080Ti as an example. It is fabricated in Ampere architecture, and requires CUDA 11.1 or later. Since we've had CUDA 11.3 loaded, pytorch with cudatoolkit 11.3 shoule be installed. 

::

    conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch

After this, install the rest packages. 

::

    conda install pandas
    conda install matplotlib
    conda install scikit-learn-intelex
    conda install numba         
    conda install tensorboard

You can check the following article to determine which CUDA to use on your GPU device. 

::

    https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/

Next, enter src/op and run the following commands to compile acceleration modules. Notice that the compilation must take place on host that has available GPU. If you are working on a cluster, use the the following to start a interactive job for compilation. 

::

    srun -p mygpupartition --gres=gpu:1 --pty bash

**You should make sure that your g++ compiler supports C++ 14 standard!** Also, you should modify the path in setup.py. It should be the bin directory in your CUDA path. To obtain the CUDA path, use 

::

    echo $CUDADIR

and the path in setup.py file should therefore be:

::

    what/echo/CUDADIR/tells/you/bin

To compile, use the following command.

::

    python3 setup.py install  

MLFF switches to use the above modules when GPU is available. However, this is a good option only for KF-DeepMD engine. For KFNN, training on GPU appears less efficient than on CPU. Certainly, we will bring modifications in future releases to better utlize the power of GPU in KFNN. We will eleborate on how to choose the computing device in following sections. 


Now, enter the src directory and compile source codes. Intel 2020 module must be loaded. 

:: 

    module load intel/2020
    cd src
    sh build.sh
    

After compilation, you should modify environment variables. The absolute path of src/bin should be exported in ~/.bashrc. You can use "echo $PWD" to obtain the absolute path.

::

    vim ~/.bashrc 
    export PATH=absolute/path/of/src/bin:$PATH
    source ~/.bashrc 

Prepare training data 
---------------------

The format of training data is same as the MOVEMENT files generated by PWmat's molecular dynamics caculation. Please refer to PWmat manual for details involving MD calculations. **Notice that it is very important to set energy_decomp = T in etot.input**, otherwise there will be no feature to extract. 

A sample etot.input is shown below. 

:: 

    16  1
    JOB = MD
    IN.PSP1 = Cu.SG15.PBE.UPF
    IN.PSP2 = Au.SG15.PBE.UPF
    IN.ATOM = atom.config
    MD_DETAIL = 3 400 0.8 300 300
    E_Cut = 60
    precision = double
    energy_decomp = T       #this flag must be true
    mp_n123 = 1 1 1 0 0 0 2
    xcfunctional = GGA
    E_error = 1.0e-6
    Rho_error = 1.0e-4

The resultant MOVEMENT files should be combined into one single file. MOVEMENT files can comes from the same kind of system with different atom number. Use the following command to do so. 

::

    cat MOVEMENT_1 MOVEMENT_2 ... > MOVEMENT 

Names other than "MOVEMENT" are not allowed.  

**Principles for generating trianing data**

As the first principle, training data set should well represent the 3N-dimensional phase space, where N is the number of atoms. That is, data should include the system’s spatial configurations as many as possible. The reason is sell-evident under the framework of energy decomposition. In our example, the training data is usually made up of images from more several MD results with varying condtitions. However, these images are sampled from the raw data, otherwise data size can be overwhelming. We now use some naïve rules to pick up images from the raw data. We may introduce more complex sampling method in the future. 

Configuration and features generation  
-------------------------------------

First, export the absolute path to src/bin in the ~/.bashrc. You can obtain the absolute path via command "pwd". 

::

    cd src/bin
    pwd
    (copy the absolute path)
    vim ~/.bashrc
    export PATH=/absolute/path/to/bin:$PATH

Create a new directory (call it examples) that will contain all the cases. This directory should be created in the directory that contains README.md. Enter directory examples, and create a new directory for a single system. In our exmaple, we study Copper, whose directory is called Cu1646. 


In Cu1646, create a directory callled **PWdata** and move the MOVEMENT file in it. A parameters.py file should appear in the same directory. An environmental configuration also has to be done.

**codedir**: the absolute path of the MLFF package, which is the one that contains directory src. Notice that letter r must appear in front of the path string. This step 

::
    
    codedir=r'/your/path/to/MLFF_torch'

For feature generation, the folllowing parameters should be set correctly. 

**atomType**: the atomic numbers. In the example case, system consists of only Cu, thus atomType should be [29]. If the system contains more than one element, all atomic numbers should be specified. For instance, atomType should be [8,29] for CuO. Order does not matter here. 

**use_Ftype**: features fed into the training process. 8 types of features are provided, which are 

        1. 2-body(2b)

        2. 3-body(3b) 

        3. 2-body Gaussian(2bgauss)

        4. 3-body Cosine(3bcos) 

        5. Multiple Tensor Potential(MTP)

        6. Spectral Neighbor Analysis Potential(SNAP)

        7. deepMD-Chebyshev(deepMD1)
        
        8. deepMD-Gaussian(deepMD2) 

Please refer to Theoretical Backgrounds section for more details. Usually, combinations such as [1,2],[3,4],[5],[6],[7],[8] are used, but you are free to explore other combinations. In the given example, we use [1,2]. Note that feature 6 could be slow. 

**isCalcFeat**: set to be True. Notice that this step will generate feature output files that can be reused by other training processes. They are stored in directory fread_dfeat. 

**Rc_M**: the cutoff radius of feature generation, in Angstrom. Since all of our 8 features are "local", which assumes that atomic properties such as energy are determined by near neighbors, this parameter controls how many neighbors are taken into account when generating features. Its default value is 6, but we recommand you trying different values for different system. 

**maxNeighborNum**: its default value is 100, so you can try it with altering. However, for some system it is not enough to accommodate all the neighbors, and the feature generation fails. The singal of such an error can be found in /output. For each feature, an out file is generated. There should be out1 and out2 if feature combination [1,2] is chosen. In each out file, feature generation detail of each MD step is recorded. The correct scenario is shown below. 

.. image:: pictures/feature_success.png

If, however, you find that no information was printed, like the scenario shown below, you shoud assign **maxNeighborNum** with a larger number. 

.. image:: pictures/feature_fail.png 

After parameters are all set, run mlff.py to obtain the features. 
::
    
    mlff.py

Having generated the feature data, you can now feed them in various training engines. **isCalcFeat** should be turned off now. 

Engine 1: Linear Model
----------------------

1.Training
^^^^^^^^^^

Turn on **isFitLinModel** to lanuch linear fitting. After training, turn off **isFitLinModel**. 

2.Inference
^^^^^^^^^^^

After training, you can use the model to run MD calculation in an alternative data set. We call this step inference. Prepare another Ab Initio MOVEMENT file. Create a new directory called MD and move another MOVEMENT into it.  

Several parameters should be set. 

**isNewMd100**: set True

**imodel**: set to be 1, which is linear model. 

**md_num_process**: the mpi process number you wish to use. Its value can be up to the number of available cores in you CPU. 

Next, run mlff.py. You may also use the bash file we provided to submit a mlff job. 

::
    
    mlff.py

A sample slurm script is given below. Notice that when submitting jobs through slurm, ntasks-per-node determines how many cores you can use. 

::

    #!/bin/sh
    #SBATCH --partition=mypartition
    #SBATCH --job-name=cu1646_l12
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=1
    #SBATCH --threads-per-core=1

    conda activate mlff

    mlff.py

In our example, a new MOVEMENT file can be found after the inference step. You can copy plot_mlff_inference.py from utils/ directory to visualize the results. Below is the plot of results for Cu1646 case. 

.. image:: pictures/cu1646_linear.png


Engine 2: Nonlinear Model(VV) 
-----------------------------

VV(vector-vector) goes beyond linear fitting by introducing nonlinearity. In linear model, we approximate the total energy by a linear combination of features. But in VV, we build a new set of features from the old ones. These new features are generated by feeding old ones into nonlinear functions. For example, they could be exp(-F_i), F_i* F_i, F_i* F_i *F_i, .etc.

1.Training
^^^^^^^^^^

First, perform feature generation and fitting as in linear model. To do so, set isCalcFeat=True and isFitLinModel=True, and run mlff.py. 

After the first step, enter **fread_dfeat** directory and run 

::

    select_mm_VV.r

This routine generate secondary features based on the exisisting ones. You should input the following parameters.

**itype**: type of atom taken into account. If system only consists of 1 type of atom, input 1; if there are more than 1 type of atom, input should be 1, or 2, or 3, .etc. We will elaborate on how to deal with more than one type of atom below. 

**iseed**: a minus integer seed. It is used to randomly form a temporary training set and test set. 

**include feat**: input 0

**iscan_MM, or not**: input 1

You can observe that this routine is looping over the secondary features. Finally, 8000 secondary features are obtained. Each loop takes increasingly long time since it involves diagonalization of a dense matrix of increasing dimension. 

Next, run select_VV_MM.r again to select the best secondary features. Input parameters as follows:

**itype**: same as previous run

**iseed**: same as previous run

**include feat**: input 0

**iscan_MM, or not**: input 0

**input mm**: the number of secondary feature you wish to choose. 1000 to 2000 is a resonable range. 

For system with more than one type of element, you should run the above process more than once. For clarity, we call the two runs of select_mm_VV.r a single "selection". For each type of element, you should run selection with resepct to each element. That is, run the whole selection with **itype=1**, and next **itype=2**, **itype=3**, etc. **iseed** and **input mm** must match in each selection. 

Now, prepare a file called *select_VV.input*, which should have the following format 

::  
    
    10
    2000
    20
    0
    20,4,2.0,0.001

Except to modify the number to match your input mm parameter, you can use the rest as a template. 

After this, run feat_dist_xp.r. Choose 1 when input selection pops up. 

::

    feat_dist_xp.r

Finally, use fit_VV_forceMM.r to fit. You can observe that the numbe of feature used to fit, as well as the time to fit, significantly increased. 

::

    fit_VV_forceMM.r

2. Inference 
^^^^^^^^^^^^^

Quit from **fread_dfeat** to tyhe directory that contains parameters.py, and prepare a MD directory containing test data as introduced in Linear model section. Turn off **isCalcFeat** and **isFitLinModel** in parameters.py. Modify the following parameters to lanuch MD calculation

**isNewMd100**: set to be true 

**imodel**: 2, i.e. MD mode for VV

**md_num_process**: number of process you wish to use. 

Next, run 

::

    mlff.py

or submit job via script

::

    #!/bin/sh
    #SBATCH --partition=mycpupartition
    #SBATCH --job-name=myjobname
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=32
    #SBATCH --threads-per-core=1

    conda activate mlff_debug

    mlff.py

After MD, you make visualize the results as introduced in the linear model section. 

The graph below shows a VV inference on Cu1646 case. However, there is no guarantee that the choice of parameters is optimal. We will further explore better combinations of parameters. 

.. image:: pictures/cu1646_vv.png 

Engine 3: Kalman Filter-based Neural Network
--------------------------------------------

In this engine, we use Kalman filter to improve the bare neural network(NN). Essentially, Kalman filter smooths the “spikes” of the high dimension cost function, curbing the likelihood of falling into local minimum. 

1.Training
^^^^^^^^^^

First, several NN parameters should be set. 

**batch_size**: must be 1. We may support different batch sizes in the future. 

**nLayer** The layer of neural network. Notice that more layers does not mean better result! In our example, we set it to be 3. 

**nNode** The dimension of nodes. We have used 

::

    nNodes = np.array([[15,15],[15,15],[1,1]])

as default. Only change the first two pairs when necessary. It is also recommended not to make these number too large. 


After this, several parameters should also be set.  

**natoms** If more than one type of atom present, one should also set natoms correctly. For example, if the system of interest consists of 4 Cu atom and 7 Au atom, then you should set atomType = [29,79] and natoms = [4,7]. 

**nFeatures** It is the number of features. It should be the sum of the two numbers in the last line of   /fread_dfeat/feat.info. In our example, nFeatures is 42. 

We now use seper.py to devide data into a training set and a validation set. Currently, the division is a simple cut between first 80% and 20%. We might provide more complicated division method in the future. 

::

    seper.py

Next, use gen_data.py to re-format data. After this step you will find them in the directory train_data. 

::

    gen_data.py

Finally, set the following parameters:

**dR_neigh**: set to be False 

**use_GKalman**: set to be True

**use_LKalman**: set to be False

**is_scale**: set to be True

**itype_Ei_mean**: the estimation of mean energy of each type of atom. You should go to train_data/final_train and take a look at engy_scaled.npy via the following commands,

::

    cd train_data/final_train
    python 
    import numpy 
    numpy.load("engy_scaled.npy")

You don't need an excact mean, and a rough estimate should suffice. For example, for a CuO system which contains 2 types of atom, if the commands above returns something like this:

::

    array([[174.0633357],
       [174.0604308],
       [174.0453315],
       ...,
       [437.0013048],
       [437.3404306],
       [437.2137406]])

you can just set 

::

    itype_Ei_mean=[174.0,437.0] 

**n_epoch**: the number of epoch for training. You can start with a few hundred. 

You can now launch train.py. You should also specify a directory with flag -s to save the logs and models. As stated above, training in GPU is not efficient as in CPU at this point. To force using cpu, add **-c** flag. 
::
    
    train.py -s records -c 

You can also use scripts to submit a job on you cluster. For example, 

::
        
    #!/bin/sh
    #SBATCH --partition=mypartition
    #SBATCH --job-name=myjobname
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=num_of_threads
    #SBATCH --threads-per-core=1
    
    conda activate mlff 

    train.py -s records 

2. During training
^^^^^^^^^^^^^^^^^^

During training, you can monitor te progress by checking the logs in **records**directory. 

**epoch_loss.dat**: loss, RMSE_Etot, RMSE_Ei, RMSE_F of training set in each epoch. 

**epoch_loss_valid.dat**: RMSE_Etot, RMSE_Ei, RMSE_F of valid set in each epoch.  

**model**: directory that contains the obtained models. The latest and the best model will be saved. 

You can use -R to plug in previously trained models. It will automatically search for "latest.pt" in record/model 

::

    train.py -R  

You can compare epoch_loss.dat and epoch_loss_valid.dat to see if an overfitting occurs. 

3. Inference 
^^^^^^^^^^^^

Engine 4: Kalman Filter-based DeepMD
------------------------------------

In this module, we incorporates Kalman filter upon open source DeepMD kit. However, you may still use the DeepMD funtionalities alone without Kalman filter. 

1.Training
^^^^^^^^^^

Unlike all the other engines, you can only use 1 feature at each time. Having made sure this, modify the following parameters accordingly. 

**dR_neigh**: set to be Trues 

**use_LKalman**: set to be true if you wish to apply local Kalman filter upon deepMD. Note that **do not** attempt to use global KF, since memory usage will be unreasonably large. You should set the network configuration accordingly. See below. 

**batch_size**: without KF, batch size can be larger than 1. You can start with 4. But if KF is applied, batch size can only be 1

**n_epoch**: You need a epoch number larger than in KFNN. DeepMD might take several thousands epochs to converge. However, since a single DeepMD epoch is faster, there is no substantial difference between the total training time of DeepMD and that of KFNN. If KF is used, epoch number can be smaller. 

**nFeatures**: check the feature number in output/outx, with x being the feature index you chose. 

Having done the above, run **seper.py** and **gen_data.py** as in engine 3. 

To initiate training, you should also choose a network configuration class in accordance with the model. 

**DeepMD_cfg_dp**: without KF

**DeepMD_cfg_dp_kf**: with KF 

In trainning, pass it in as an argument after flag **-n**.

::
    
    train.py --deepmd=True -n DeepMD_cfg_dp -s record

You can also use the following script to submit job on your cluster. You have to submit this to nodes with at least 1 available GPU. 

::

    #!/bin/sh
    #SBATCH --partition=mygpupartition
    #SBATCH --job-name=cu1646_dp1
    #SBATCH --gres=gpu:1 
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=4
    #SBATCH --threads-per-core=1

    conda activate mlff

    train.py --deepmd=True -n DeepMD_cfg_dp -s record

