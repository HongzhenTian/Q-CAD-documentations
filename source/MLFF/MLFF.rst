Machine Learning Force Field
========================================

Machine Learning Force Field (MLFF) platform aims at generating force fields with accuracy comparable to Ab Initio DFT molecular dynamics. It provides 8 types of features with translation, rotation, and permutation invariance. It also supports 4 engines for training and prediction, which are: 

        1. Linear Model

        2. Nonlinear VV Model

        3. Kalman Filter-based Neural Netowrk (KFNN)

        4. Kalman Filter-based Deep Potential Model(KFDP)


Environment Setup 
-----------------

We recommmend using anaconda for package installing and management. You should first install Anaconda following the steps shown on its official website. Please don't attempt to use miniconda instead of Anaconda, since it might incur various dependecy and path errors that are hard to debug. 

To install conda environment, use the following command. You may choose a new version of Anaconda. 

::

    wget https://repo.anaconda.com/archive/Anaconda3-2020.07-Linux-x86_64.sh

Then, create a new environment for this module. 

::
    
    conda create -n mlff python=3.8

After mlff has been created, re-enter the current environment.
        
:: 
    
    conda deactivate
    conda activate mlff

After this, install the following packages. 

::

    conda install pandas
    conda install matplotlib
    conda install scikit-learn-intelex
    conda install numba         
    conda install tensorboard

Next, you should identify the architecture of your Nvidia GPU and install a compatible pytorch version. We take RTX 3080Ti as an example. It is fabricated in Ampere architecture, and requires CUDA 11.1 or later. **Also**, the one-click installation via conda only supports 4 CUDA version, which are CUDA 10.2, CUDA 11.1 CUDA 11.3 and CUDA 11.5. Thus, CUDA 11.1, CUDA 11.3 and CUDA 11.5 are reasonable choice for RTX 3080Ti. We choose to use CUDA 11.3 as an example. Therefore, you can install pytorch with the following command

::

    conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch 

The way to load a speific CUDA version differs across platforms. If you are working on a cluster, it is common to use **module load** command to load specific CUDA library. If you are working on your own workstation, unless a specific CUDA version is pre-installed, you should install it on your own. Refer to Nvidia official website for more details. 

You can check the following article to determine which CUDA to use on your GPU device.  

::

    https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/

Having configured CUDA correctly, enter src/op and run the following commands to compile acceleration modules. Notice that the compilation must take place on host that has available GPU. If you are working on a cluster, you can use the the following to start a interactive job for compilation. 

::

    srun -p mygpupartition --gres=gpu:1 --pty bash

**You should make sure that your g++ compiler supports C++ 14 standard!** Also, you should modify the path in setup.py. It should be the bin directory in your CUDA path. To obtain the CUDA path, use 

::

    echo $CUDADIR

and the path in setup.py file should therefore be:

::

    what/echo/CUDADIR/tells/you/bin

To compile, use the following command.

::

    python3 setup.py install  

MLFF switches to use the above modules when GPU is available. However, this is a good option only for KFDP engine. For KFNN, training on GPU appears less efficient than on CPU. Certainly, we will bring modifications in future releases to better utlize the power of GPU in KFNN. We will eleborate on how to choose the computing device in following sections. 


Now, enter the src directory and compile source codes. Intel 2020 module must be loaded. 

:: 

    module load intel/2020
    cd src
    sh build.sh
    

After compilation, you should modify environment variables. The absolute path of src/bin should be exported in ~/.bashrc. You can use "echo $PWD" to obtain the absolute path.

::

    vim ~/.bashrc 
    export PATH=absolute/path/of/src/bin:$PATH
    source ~/.bashrc 

Prepare training data 
---------------------

The format of training data is same as the MOVEMENT files generated by PWmat's molecular dynamics caculation. Please refer to PWmat manual for details involving MD calculations. **Notice that it is very important to set energy_decomp = T in etot.input**, otherwise there will be no feature to extract. 

A sample etot.input is shown below. 

:: 

    16  1
    JOB = MD
    IN.PSP1 = Cu.SG15.PBE.UPF
    IN.PSP2 = Au.SG15.PBE.UPF
    IN.ATOM = atom.config
    MD_DETAIL = 3 400 0.8 300 300
    E_Cut = 60
    precision = double
    energy_decomp = T       #this flag must be true
    mp_n123 = 1 1 1 0 0 0 2
    xcfunctional = GGA
    E_error = 1.0e-6
    Rho_error = 1.0e-4


Names other than "MOVEMENT" are not allowed.  

**Principles for generating trianing data**

As the first principle, training data set should well represent the 3N-dimensional phase space, where N is the number of atoms. That is, data should include the system’s spatial configurations as many as possible. The reason is sell-evident under the framework of energy decomposition. In our example, the training data is usually made up of images from more several MD results with varying condtitions. However, these images are sampled from the raw data, otherwise data size can be overwhelming. We now use some naïve rules to pick up images from the raw data. We may introduce more complex sampling method in the future. 

Configuration and features generation  
-------------------------------------

First, export the absolute path to src/bin in the ~/.bashrc. You can obtain the absolute path via command "pwd". 

::

    cd src/bin
    pwd
    (copy the absolute path)
    vim ~/.bashrc
    export PATH=/absolute/path/to/bin:$PATH

Create a new directory (call it examples) that will contain all the cases. This directory should be created in the directory that contains README.md. Enter examples, and create a new directory for a single system. In our exmaple, we study Copper, whose directory is called Cu1646. 

In Cu1646, create a directory callled **PWdata**. In PWdata, **create a single directory for each MOVEMENT file you wish to train.**, and move all the MOVEMENT files in their corresponding directory. Name of the directory does not matter here. It is very important to put multiple MOVEMENT files in seperate directories: that being said, do not concatenate multiple MOVEMENT file into one. This is because in seper.py, a simple 80%-20% cut is used to form the training set and the validation set. You will probably end up with having a case that is not trained at all and only used as validation data! 

A parameters.py file should appear in the same directory in which **PWdata** resides. An environmental configuration also has to be done. 

**codedir**: the absolute path of the MLFF package, which is the one that contains directory src. Notice that letter r must appear in front of the path string. This step 

::
    
    codedir=r'/your/path/to/MLFF_torch'

For feature generation, the folllowing parameters should be set correctly. 

**atomType**: the atomic numbers. In the example case, system consists of only Cu, thus atomType should be [29]. If the system contains more than one element, all atomic numbers should be specified. For instance, atomType should be [8,29] for CuO. Order does not matter here. 

**use_Ftype**: features fed into the training process. 8 types of features are provided, which are 

        1. 2-body(2b)

        2. 3-body(3b) 

        3. 2-body Gaussian(2bgauss)

        4. 3-body Cosine(3bcos) 

        5. Multiple Tensor Potential(MTP)

        6. Spectral Neighbor Analysis Potential(SNAP)

        7. Deep Potential-Chebyshev(dp1)
        
        8. Deep Potential-Gaussian(dp2) 

Please refer to Theoretical Backgrounds section for more details. Usually, combinations such as [1,2],[3,4],[5],[6],[7],[8] are used, but you are free to explore other combinations. In the given example, we use [1,2]. Note that feature 6 could be slow. 

**isCalcFeat**: set to be True. Notice that this step will generate feature output files that can be reused by other training processes. They are stored in directory fread_dfeat. 

**Rc_M**: the cutoff radius of feature generation, in Angstrom. Since all of our 8 features are "local", which assumes that atomic properties such as energy are determined by near neighbors, this parameter controls how many neighbors are taken into account when generating features. Its default value is 6, but we recommand you trying different values for different system. 

**maxNeighborNum**: its default value is 100. However, for some systems it is not enough to accommodate all the neighbors, and thus the feature generation fails. The singal of such an error can be found in /output. For each feature, an out file is generated. There should be out1 and out2 if feature combination [1,2] is chosen. In each out file, feature generation detail of each MD step is recorded. The correct scenario is shown below. 

.. image:: pictures/feature_success.png

If, however, you find that no information was printed, like the scenario shown below, you shoud assign **maxNeighborNum** with a larger number. 

.. image:: pictures/feature_fail.png 

After parameters are all set, run mlff.py to obtain the features. 
::
    
    mlff.py


Having generated the feature data, you can now feed them in various training engines. **isCalcFeat** should be turned off now. 

Engine 1: Linear Model
----------------------

1.Training
^^^^^^^^^^

Turn on **isFitLinModel** to lanuch linear fitting. After training, turn off **isFitLinModel**. 

2.Inference
^^^^^^^^^

After training, you can use the model to run MD calculation. We call this step inference. There are two kinds of inference: test and prediction. In test, one first prepare a MOVEMENT file generated by Ab Initio calculation, use the obtained force field to calculate energy and force, and compare them against the Ab Initio results. This step can be seen as a more rigorous assessment of the quality of the force field. 

In comparison, prediction solves real challenges. Like a Ab Initio MD calculation, it starts with a initial image, and simulates the ensuing process based on the force field. 

Test
""""

Prepare another Ab Initio MOVEMENT file. Create a new directory called MD and move another MOVEMENT into it.  

Several parameters should be set. 

**isNewMd100**: set True

**imodel**: set to be 1, which is linear model. 

**md_num_process**: the mpi process number you wish to use. Its value can be up to the number of available cores in you CPU. 

Next, run mlff.py. You may also use the bash file we provided to submit a mlff job. 

::
    
    mlff.py

A sample slurm script is given below. Notice that when submitting jobs through slurm, ntasks-per-node determines how many cores you can use. 

::

    #!/bin/sh
    #SBATCH --partition=mypartition
    #SBATCH --job-name=cu1646_l12
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=1
    #SBATCH --threads-per-core=1

    conda activate mlff

    mlff.py

In our example, a new MOVEMENT file can be found after the inference step. You can copy plot_mlff_inference.py from utils/ directory to visualize the results. Below is the plot of results for Cu1646 case. 

.. image:: pictures/lft.png


Prediction
""""""""""

The process of prediction is almost identical to that of MD in PWmat. First, create a **atom.config** file that contains an initial image. Next modify **md.input** generated **test** step. Such a file should appear like this: 
::

    md.atom.config
    1, 100, 1.0, 600, 600
    F
    1
    1
    1
    29 58

You should modify the first and the second line according to your need. The first line specifies the input file of the initial image. The second line is the same as **MD_DETAIL** flag in PWmat MD calculation. Please refer to PWmat's user manual for details. 

Use the following command to run prediction after parameters are set. **NUM_THREAD** stands for the mpi process number you wish to use. 

::

    mpirun -n NUM_THREAD main_MD.x 

The outputs of prediction are the same as those of PWmat MD calculation. 

Engine 2: Nonlinear Model(VV) 
-------------------------

VV(vector-vector) goes beyond linear fitting by introducing nonlinearity. In linear model, we approximate the total energy by a linear combination of features. But in VV, we build a new set of features from the old ones. These new features are generated by feeding old ones into nonlinear functions. For example, they could be exp(-F_i), F_i* F_i, F_i* F_i *F_i, .etc.

1.Training
^^^^^^^^^^

First, perform feature generation and fitting as in linear model. To do so, set isCalcFeat=True and isFitLinModel=True, and run mlff.py. 

After the first step, enter **fread_dfeat** directory and run 

::

    select_mm_VV.r

This routine generate secondary features based on the exisisting ones. You should input the following parameters.

**itype**: type of atom taken into account. If system only consists of 1 type of atom, input 1; if there are more than 1 type of atom, input should be 1, or 2, or 3, .etc. We will elaborate on how to deal with more than one type of atom below. 

**iseed**: a minus integer seed. It is used to randomly form a temporary training set and test set. 

**include feat**: input 0

**iscan_MM, or not**: input 1

You can observe that this routine is looping over the secondary features. Finally, 8000 secondary features are obtained. Each loop takes increasingly long time since it involves diagonalization of a dense matrix of increasing dimension. 

Next, run select_VV_MM.r again to select the best secondary features. Input parameters as follows:

**itype**: same as previous run

**iseed**: same as previous run

**include feat**: input 0

**iscan_MM, or not**: input 0

**input mm**: the number of secondary feature you wish to choose. 1000 to 2000 is a resonable range. 

For system with more than one type of element, you should run the above process more than once. For clarity, we call the two runs of select_mm_VV.r a single "selection". For each type of element, you should run selection with resepct to each element. That is, run the whole selection with **itype=1**, and next **itype=2**, **itype=3**, etc. **iseed** and **input mm** must match in each selection. 

Now, prepare a file called *select_VV.input*, which should have the following format 

::  
    
    10
    2000
    20
    0
    20,4,2.0,0.001

Except to modify the number to match your input mm parameter, you can use the rest as a template. 

After this, run feat_dist_xp.r. Choose 1 when input selection pops up. 

::

    feat_dist_xp.r

Finally, use fit_VV_forceMM.r to fit. You can observe that the numbe of feature used to fit, as well as the time to fit, significantly increased. 

::

    fit_VV_forceMM.r

2. Inference 
^^^^^^^^^^^^^

**Test**

Quit from **fread_dfeat** to tyhe directory that contains parameters.py, and prepare a MD directory containing test data as introduced in Linear model section. Turn off **isCalcFeat** and **isFitLinModel** in parameters.py. Modify the following parameters to lanuch MD calculation

**isNewMd100**: set to be true 

**imodel**: 2, i.e. MD mode for VV

**md_num_process**: number of process you wish to use. 

Next, run 

::

    mlff.py

or submit job via script

::

    #!/bin/sh
    #SBATCH --partition=mycpupartition
    #SBATCH --job-name=myjobname
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=32
    #SBATCH --threads-per-core=1

    conda activate mlff_debug

    mlff.py

After MD, you make visualize the results as introduced in the linear model section. 

The graph below shows a VV inference on Cu1646 case. However, there is no guarantee that the choice of parameters is optimal. We will further explore better combinations of parameters. 

.. image:: pictures/vv.png 

**Prediction**

The procedure is similar to that of linear model. 

Engine 3: Kalman Filter-based Neural Network
--------------------------------------------

In this engine, we use Kalman filter to improve the bare neural network(NN). Essentially, Kalman filter smooths the “spikes” of the high dimension cost function, curbing the likelihood of falling into local minimum. 

1.Training
^^^^^^^^^^

First, several NN parameters should be set. 

**batch_size**: must be 1. We may support different batch sizes in the future. 

**nLayer** The layer of neural network. Notice that more layers does not mean better result! In our example, we set it to be 3. 

**nNode**: Number of nodes in each layer. In our example we have the following network setting:

::

    nNodes = np.array([[15],[15],[1]]) 

This means the first and the second layer have 15 nodes each, and the final layer is the output layer with only 1 node. 

If the system has more than one type of element, each type should be assigned with a network. For exmaple, for a system with 2 types of element, set up the networks in the following manner: 

::

    nNodes = np.array([[15,15],[15,15],[1,1]]) 

You can adjust the network size according to your need. Be advised, however, that due to the heavy computation required by KF, node number per atom should not be too large, and 15 appears reasonable in our test. 

After this, several parameters should also be set.  

**natoms** If more than one type of atom present, one should also set natoms correctly. For example, if the system of interest consists of 4 Cu atom and 7 Au atom, then you should set atomType = [29,79] and natoms = [4,7]. 

We now use seper.py to devide data into a training set and a validation set. Currently, the division is a simple cut between first 80% and 20%. We might provide more complicated division method in the future. Run the following command in the same directory. 

::

    seper.py

Next, use gen_data.py to re-format data. After this step you will find them in the directory train_data. 

::

    gen_data.py

Finally, set the following parameters:

**nFeatures** It is the number of features. It should be the sum of the two numbers in the last line of   /fread_dfeat/feat.info. In our example, nFeatures is 42. 

**dR_neigh**: set to be False 

**use_GKalman**: set to be True

**use_LKalman**: set to be False

**is_scale**: set to be True

**storage_scaler**: set to be True. **This is important since it saves the scaler of data for later MD runs.** 

**itype_Ei_mean**: the estimation of mean energy of each type of atom. You should go to train_data/final_train and take a look at engy_scaled.npy via the following commands,

::

    cd train_data/final_train
    python 
    import numpy 
    numpy.load("engy_scaled.npy")

You don't need an excact mean, and a rough estimate should suffice. For example, for a CuO system which contains 2 types of atom, if the commands above returns something like this:

::

    array([[174.0633357],
       [174.0604308],
       [174.0453315],
       ...,
       [437.0013048],
       [437.3404306],
       [437.2137406]])

you can just set 

::

    itype_Ei_mean=[174.0,437.0] 

**n_epoch**: the number of epoch for training. You can start with a few hundred. 

You can now launch train.py. You should also specify a directory with flag -s to save the logs and models. As stated above, training in GPU is not efficient as in CPU at this point. To force using cpu, add **--cpu** flag. 
::
    
    train.py -s records --cpu

You can also use scripts to submit a job on you cluster. For example, 

::
        
    #!/bin/sh
    #SBATCH --partition=mypartition
    #SBATCH --job-name=myjobname
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=num_of_threads
    #SBATCH --threads-per-core=1
    
    conda activate mlff 

    train.py -s records 

2. During training
^^^^^^^^^^^^^^^^^^

During training, you can monitor te progress by checking the logs in **records**directory. 

**epoch_loss.dat**: loss, RMSE_Etot, RMSE_Ei, RMSE_F of training set in each epoch. 

**epoch_loss_valid.dat**: RMSE_Etot, RMSE_Ei, RMSE_F of valid set in each epoch.  

**model**: directory that contains the obtained models. The latest and the best model will be saved. 

You can use -R to plug in previously trained models. It will automatically search for "latest.pt" in record/model 

::

    train.py -R  

You can compare epoch_loss.dat and epoch_loss_valid.dat to see if an overfitting occurs. 

3. Inference 
^^^^^^^^^^^^

**Test**

Copy **read_torch_wij.py** from diretory utils to the directory you are working in. Also, copy the compiled executable **main_MD.x** in QCAD/fortran_code into **src/bin**. 

Run 

::

    python3 read_torch_wij.py

in your working directory. You should find **Wij.txt** and **data_scaler.txt** in /fread_dfeat after this step. 

Next, set the following parameters in parameters.py 

**isNewMd100**: set to be true 

**imodel**: 3, i.e. MD mode for NN

**md_num_process**: number of process you wish to use. 

Next, run 

::

    mlff.py

or submit job via script

::

    #!/bin/sh
    #SBATCH --partition=mycpupartition
    #SBATCH --job-name=myjobname
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=32
    #SBATCH --threads-per-core=1

    conda activate mlff_debug

    mlff.py

This step is similar to the MD calculation in PWmat. After this, you can find a MOVEMENT file in the currently directory, which is generated by the MLFF-MD calculation. Copy plot_nn_test.py from /utils to the current working directory. 

Run

::

    python3 plot_mlff_test.py 

to generate plot of inference result. The following plot shows the KFNN inference result on cu1646 case. 

.. image:: pictures/nn.png  

**Prediction**

The procedure for KFNN prediction is the same as in the linear model. 
 
Engine 4: Kalman Filter-based Deep Potential
---------------------------

In this module, we incorporates Kalman filter upon Deep Potential model. You may choose to run DP with or without Kalman filter. 

1.Training
^^^^^^^^^^

Deep Potential model does not require input feature. You can use feature 1 as a placeholder. Next, set the following parameters. 

**dR_neigh**: set to be True

**use_LKalman**: set to be true if you wish to apply local Kalman filter upon DP. Note that **do not** attempt to use global KF, since memory usage will be unreasonably large. You should set the network configuration accordingly. See below. 

**batch_size**: without KF, batch size can be larger than 1. You can start with 4. But if KF is applied, batch size can only be 1

**n_epoch**: You need a epoch number larger than in KFNN. DP might take several thousands epochs to converge. However, since a single DP epoch is faster, there is no substantial difference between the total training time of DP and that of KFNN. If KF is used, epoch number can be smaller. 

**nFeatures**: check the feature number in output/outx, with x being the feature index you chose. 

Having done the above, run **seper.py** and **gen_data.py** as in engine 3. 

To initiate training, you should also choose a network configuration class in accordance with the model. 

**DP_cfg_dp**: without KF

**DP_cfg_dp_kf**: with KF 

In trainning, pass it in as an argument after flag **-n**.

::
    
    train.py --dp=True -n DP_cfg_dp -s record

You can also use the following script to submit job on your cluster. You have to submit this to nodes with at least 1 available GPU. 

::

    #!/bin/sh
    #SBATCH --partition=mygpupartition
    #SBATCH --job-name=cu1646_dp1
    #SBATCH --gres=gpu:1 
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=4
    #SBATCH --threads-per-core=1

    conda activate mlff

    train.py --dp=True -n DP_cfg_dp -s record 


2. Inference 
^^^^^^^^^^^^

**Test** 

Move all the files in **PWdata** to a backup directory, and copy MOVEMENT file you wish to test against into **PWdata**. Use **mlff.py** to process data as in training. Next, modify the following parameters in parameters.py:

**test_ratio**: set to 1. This means that all images are used for testing. 

**prediction_path**: output path for data generation. 

**model_path**: path to the .pt file. It is stored in /record/model

Now, perform data processing as in trianing: 

::

    mlff.py
    seper.py
    gen_data.py

Finally, run 

::
    
    test.py --dp=True -n DP_cfg_dp

RMSE of total energy and force will be reported at the end. 

**Prediction**

First, backing up all the files in /PWdata. Create 2 files for MD calculation:

**atom.config**: The initial image for MD calculation. 

**md.input**: The input file for MD calculation. For Deep Potential model prediction, this file should look like this:

::

    atom.config             (input file name)
    1, 100, 1.0, 600, 600   (MD_DETAILS in PWmat MD calculation)
    F                       (Place holder)
    4                       (Type of model, 4 stands for dp)
    1                       (interval for MD movement. No need to change)
    1                       (Types of atom)
    29 58                   (Atomic number and mass)

Finally, use main_MD.x to start calculation. Notice that for this model, do not use mpirun 

::

    main_MD.x

